{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentenceSplitting.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SmAwF09-aq-"
      },
      "source": [
        "Early simple rule-based model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncQmV9nBsr-A"
      },
      "source": [
        "imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0lYnG6usv6Y"
      },
      "source": [
        "def separate(sentence):\n",
        "  #takes in a sentence, splits it into the subsentences\n",
        "  tokens = sentence.split() # or  nltk.word_tokenize(sentence)\n",
        "  current = []\n",
        "  sentences = []\n",
        "  for i, word in enumerate(tokens, 0):\n",
        "    if word in [\"then\", \"followed\"]:\n",
        "      sentences.append(' '.join(current))\n",
        "      current = []\n",
        "    elif word == \"and\" and tokens[i+1] in ['a', 'an', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']:\n",
        "      sentences.append(' '.join(current))\n",
        "      current = []\n",
        "    else:\n",
        "      current.append(word)\n",
        "    sentences.append(' '.join(current))\n",
        "    return sentences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylkj9ZsXsjDS"
      },
      "source": [
        "Neural network based model\n",
        "\n",
        "This is based on the code we used in the sentence to waypoint models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtDjfwfu-nQ-"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPQQARJE-ova"
      },
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from matplotlib import pyplot as plt\n",
        "#from transformers import BertModel, BertConfig\n",
        "from google.colab import files\n",
        "files.upload()  # load dataGeneration.py\n",
        "\n",
        "from splittingDataGenerator import generateTestBatch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r28iSWj-129"
      },
      "source": [
        "# import the pretrained models\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"bert-base-uncased\")\n",
        "baseModel = model.bert\n",
        "#config = BertConfig()\n",
        "#baseModel = BertModel(config)\n",
        "\n",
        "# helper for padding 2D lists into tensors\n",
        "def listToTensor(list):\n",
        "  maxLen = max(len(l) for l in list)\n",
        "  tensor = torch.tensor([l + (maxLen - len(l)) * [0] for l in list])\n",
        "  return tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCEkR6W6QI7y"
      },
      "source": [
        "# custom head for BERT\n",
        "# credit to Martin Zablocki: https://zablo.net/blog/post/custom-classifier-on-bert-model-guide-polemo2-sentiment-analysis/\n",
        "class SplittingNet(nn.Module):\n",
        "  def __init__(self, base_model, n_classes=30, base_model_output_size=768, hidden_size = 100, dropout=0.05):\n",
        "    super().__init__()\n",
        "    #self.base_model = base_model\n",
        "    self.bert = base_model\n",
        "    #self.linear = nn.Linear(hidden_size, 30)\n",
        "    #self.dropout = nn.Dropout(dropout)\n",
        "    self.lstm = nn.LSTM(input_size=base_model_output_size, hidden_size=hidden_size,\n",
        "                        num_layers=2, dropout=dropout)\n",
        "    #self.lstm2 = nn.LSTM(input_size)\n",
        "    self.linear = nn.Linear(100, 30)\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(base_model_output_size, hidden_size),\n",
        "            nn.PReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, n_classes),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "    \n",
        "    for layer in self.classifier:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if layer.bias is not None:\n",
        "                    layer.bias.data.zero_()\n",
        "\n",
        "  def forward(self, input, *args):\n",
        "    X, attention_mask = input\n",
        "    hidden_states = self.bert(X, token_type_ids=None, attention_mask = attention_mask)\n",
        "    #print(hidden_states.last_hidden_state.shape)\n",
        "    \n",
        "    #output = self.classifier(hidden_states.last_hidden_state[:,0,:])\n",
        "    output = self.lstm(hidden_states.last_hidden_state.permute(1,0,2))[0].permute(1,0,2)\n",
        "    #print(output.shape)\n",
        "    output = self.linear(output[:,-1,:])\n",
        "    #print(output.shape)\n",
        "    return output\n",
        "    \n",
        "\n",
        "  #def forward(self, input, *args):\n",
        "  #  X, attention_mask = input\n",
        "  #  hidden_states = self.bert(X, attention_mask = attention_mask)[0]\n",
        "  #  lstm_out = self.lstm(hidden_states.permute(1,0,2))[0].permute(1,0,2)\n",
        "  #  return self.linear(lstm_out[:, -1, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbWB3CzKZjzF"
      },
      "source": [
        "This is where it trains"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEMI4WV8ZUUv"
      },
      "source": [
        "# training model\n",
        "if torch.cuda.is_available():  \n",
        "  #device = torch.device(\"cpu\")\n",
        "  device = torch.device(\"cuda:0\") \n",
        "else:  \n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "net = SplittingNet(baseModel)\n",
        "net = net.to(device=device)\n",
        "criterion = nn.MultiLabelSoftMarginLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "objectiveValues = []\n",
        "running_loss = 0\n",
        "\n",
        "# each epoch is a newly generated batch of test data\n",
        "for epoch in range(7000):\n",
        "  text, labels = generateTestBatch(100)\n",
        "  labels = torch.tensor(labels, dtype = int)\n",
        "\n",
        "  enc = tokenizer.batch_encode_plus(text)\n",
        "  X = listToTensor(enc[\"input_ids\"])\n",
        "  attn = listToTensor(enc[\"attention_mask\"])\n",
        "\n",
        "  X = X.to(device)\n",
        "  attn = attn.to(device)\n",
        "  labels = labels.to(device)\n",
        "  #labels=labels.to(torch.float32)\n",
        "  #labels.type(torch.LongTensor)\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # forward + backward + optimize\n",
        "  outputs = net((X, attn))\n",
        "  if epoch == 0:\n",
        "    print(outputs.shape)\n",
        "    print(labels.shape)\n",
        "  #print(outputs.shape)\n",
        "  #outputs.type(torch.LongTensor)\n",
        "  #print(labels.shape)\n",
        "  loss = criterion(outputs, labels)\n",
        "  #loss.type(torch.LongTensor)\n",
        "  loss=loss.to(torch.float32)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # print statistics\n",
        "  running_loss += loss.item()\n",
        "  if ((epoch + 1) % 100 == 0):\n",
        "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / 100))\n",
        "    running_loss = 0\n",
        "  objectiveValues.append(loss.item())\n",
        "\n",
        "plt.plot(objectiveValues)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qLG7ZGlyPLnk",
        "outputId": "56b1e842-df9e-4bc0-e2c5-76017b453227"
      },
      "source": [
        "for epoch in range(7000):\n",
        "  text, labels = generateTestBatch(100)\n",
        "  labels = torch.tensor(labels, dtype = int)\n",
        "\n",
        "  enc = tokenizer.batch_encode_plus(text)\n",
        "  X = listToTensor(enc[\"input_ids\"])\n",
        "  attn = listToTensor(enc[\"attention_mask\"])\n",
        "\n",
        "  X = X.to(device)\n",
        "  attn = attn.to(device)\n",
        "  labels = labels.to(device)\n",
        "  #labels=labels.to(torch.float32)\n",
        "  #labels.type(torch.LongTensor)\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # forward + backward + optimize\n",
        "  outputs = net((X, attn))\n",
        "  if epoch == 0:\n",
        "    print(outputs.shape)\n",
        "    print(labels.shape)\n",
        "  #print(outputs.shape)\n",
        "  #outputs.type(torch.LongTensor)\n",
        "  #print(labels.shape)\n",
        "  loss = criterion(outputs, labels)\n",
        "  #loss.type(torch.LongTensor)\n",
        "  loss=loss.to(torch.float32)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # print statistics\n",
        "  running_loss += loss.item()\n",
        "  if ((epoch + 1) % 100 == 0):\n",
        "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / 100))\n",
        "    running_loss = 0\n",
        "  objectiveValues.append(loss.item())\n",
        "\n",
        "plt.plot(objectiveValues)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([101, 30])\n",
            "torch.Size([101, 30])\n",
            "[100] loss: 0.083\n",
            "[200] loss: 0.082\n",
            "[300] loss: 0.082\n",
            "[400] loss: 0.082\n",
            "[500] loss: 0.081\n",
            "[600] loss: 0.081\n",
            "[700] loss: 0.081\n",
            "[800] loss: 0.080\n",
            "[900] loss: 0.081\n",
            "[1000] loss: 0.080\n",
            "[1100] loss: 0.079\n",
            "[1200] loss: 0.078\n",
            "[1300] loss: 0.079\n",
            "[1400] loss: 0.077\n",
            "[1500] loss: 0.076\n",
            "[1600] loss: 0.076\n",
            "[1700] loss: 0.076\n",
            "[1800] loss: 0.075\n",
            "[1900] loss: 0.074\n",
            "[2000] loss: 0.074\n",
            "[2100] loss: 0.073\n",
            "[2200] loss: 0.072\n",
            "[2300] loss: 0.073\n",
            "[2400] loss: 0.071\n",
            "[2500] loss: 0.071\n",
            "[2600] loss: 0.070\n",
            "[2700] loss: 0.069\n",
            "[2800] loss: 0.069\n",
            "[2900] loss: 0.068\n",
            "[3000] loss: 0.066\n",
            "[3100] loss: 0.065\n",
            "[3200] loss: 0.065\n",
            "[3300] loss: 0.065\n",
            "[3400] loss: 0.063\n",
            "[3500] loss: 0.062\n",
            "[3600] loss: 0.061\n",
            "[3700] loss: 0.060\n",
            "[3800] loss: 0.059\n",
            "[3900] loss: 0.059\n",
            "[4000] loss: 0.058\n",
            "[4100] loss: 0.057\n",
            "[4200] loss: 0.056\n",
            "[4300] loss: 0.055\n",
            "[4400] loss: 0.054\n",
            "[4500] loss: 0.053\n",
            "[4600] loss: 0.053\n",
            "[4700] loss: 0.052\n",
            "[4800] loss: 0.051\n",
            "[4900] loss: 0.050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-2343d5fb3880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m#loss.type(torch.LongTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l66jU20kZ4PQ"
      },
      "source": [
        "# save model\n",
        "torch.save(net.state_dict(), \"splittingBERT.pth\")\n",
        "files.download(\"splittingBERT.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "novodshxOhev"
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = 'REPLACE_WITH_YOUR_FILE_ID'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvjYSMMqO1hv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}